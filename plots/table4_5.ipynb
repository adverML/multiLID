{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b0be57e-6e73-404d-b94c-0c083affa5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from numpy import random\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "home = str(Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "457e3049-f18a-48af-9682-644787480f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(clf, X_test, scaler=None):\n",
    "    if not scaler == None:\n",
    "        X_test   = scaler.transform(X_test)\n",
    "    y_hat    = clf.predict(X_test)\n",
    "    y_hat_pr = clf.predict_proba(X_test)[:, 1]\n",
    "    return y_hat, y_hat_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f6bae7c8-dfac-4179-88a2-06c1b5137130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X_train, y_train, X_test, y_test):\n",
    "    scaler  = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    \n",
    "    clf = LogisticRegressionCV(max_iter=1000, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat, y_hat_pr = predictions(clf, X_test, scaler)\n",
    "    \n",
    "    return clf, scaler, y_hat, y_hat_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "b8323319-9c2f-4a2c-ae43-032b67edb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(X_train, y_train, X_test, y_test):\n",
    "    clf = RandomForestClassifier(n_estimators=300, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat, y_hat_pr = predictions(clf, X_test)\n",
    "    \n",
    "    return clf, y_hat, y_hat_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a36e1077-1541-4a38-a854-cafeb0a92e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(nor, adv, factor=0.8):\n",
    "    if len(nor.shape) > 2: \n",
    "        nor = nor.reshape((nor.shape[0], -1))\n",
    "        adv = adv.reshape((adv.shape[0], -1))\n",
    "\n",
    "    y_nor = np.zeros(nor.shape[0]).astype('int')\n",
    "    y_adv = np.ones(adv.shape[0]).astype('int')\n",
    "\n",
    "    x_train_n, x_test_n, y_train_n, y_test_n = train_test_split(nor, y_nor, test_size=1-factor, train_size=factor, random_state=random_state)\n",
    "    x_train_a, x_test_a, y_train_a, y_test_a = train_test_split(adv, y_adv, test_size=1-factor, train_size=factor, random_state=random_state)\n",
    "\n",
    "    X_train = np.concatenate((x_train_n, x_train_a))\n",
    "    y_train = np.concatenate((y_train_n, y_train_a))\n",
    "\n",
    "    X_test = np.concatenate((x_test_n, x_test_a))\n",
    "    y_test = np.concatenate((y_test_n, y_test_a))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e8d54-85f9-4358-859b-26c5fc74e71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "factor = 0.6\n",
    "nr_mean = 2\n",
    "mean_tables = []\n",
    "nr_samples=2000\n",
    "\n",
    "attacks = ['fgsm', 'bim', 'pgd', 'aa', 'df', 'cw']\n",
    "datasets = ['cifar10', 'cifar100', 'imagenet']\n",
    "models = ['wrn28-10', 'vgg16', 'wrn50-2']\n",
    "# detectors = ['lid', 'multilid']\n",
    "detectors = ['multilid']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for it in range(nr_mean):\n",
    "    random_state = [21, 30, 65][it] # random.randint(100)\n",
    "    final_table = np.zeros((len(attacks), 2))\n",
    "    base_path = os.path.join(home, 'workspace/multiLID/data/extract/run_{}/'.format(it+1))\n",
    "    results[it] = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        if dataset == 'imagenet':\n",
    "            k=30\n",
    "        else:\n",
    "            k=20\n",
    "\n",
    "        for model in models:\n",
    "            if dataset in ['cifar10', 'cifar100'] and model in ['wrn50-2']:\n",
    "                continue\n",
    "            if dataset in ['imagenet'] and model in  ['wrn28-10', 'vgg16']:\n",
    "                continue\n",
    "\n",
    "            if not dataset in results[it]:\n",
    "                results[it][dataset] = {}\n",
    "            if not model in results[it][dataset]:\n",
    "                results[it][dataset][model] = {}\n",
    "\n",
    "            for detector in detectors:\n",
    "                nor_fgsm  = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/fgsm/k{k}/{detector}_normalos_8255.pt\"))[:nr_samples]\n",
    "                adv_fgsm  = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/fgsm/k{k}/{detector}_adverlos_8255.pt\"))[:nr_samples]\n",
    "                nor_bim   = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/bim/k{k}/{detector}_normalos_8255.pt\"))[:nr_samples]\n",
    "                adv_bim   = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/bim/k{k}/{detector}_adverlos_8255.pt\"))[:nr_samples]\n",
    "                nor_pgd   = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/pgd/k{k}/{detector}_normalos_8255.pt\"))[:nr_samples]\n",
    "                adv_pgd   = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/pgd/k{k}/{detector}_adverlos_8255.pt\"))[:nr_samples]\n",
    "                nor_aa    = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/aa/k{k}/{detector}_normalos_8255.pt\"))[:nr_samples]\n",
    "                adv_aa    = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/aa/k{k}/{detector}_adverlos_8255.pt\"))[:nr_samples]\n",
    "                nor_df    = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/df/k{k}/{detector}_normalos.pt\"))[:nr_samples]\n",
    "                adv_df    = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/df/k{k}/{detector}_adverlos.pt\"))[:nr_samples]\n",
    "                nor_cw    = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/cw/k{k}/{detector}_normalos.pt\"))[:nr_samples]\n",
    "                adv_cw    = torch.load(os.path.join(base_path,  f\"{dataset}/{model}/{detector}/cw/k{k}/{detector}_adverlos.pt\"))[:nr_samples]\n",
    "                \n",
    "                assert(nor_fgsm.shape[0] == nr_samples)\n",
    "                assert(nor_bim.shape[0] == nr_samples)\n",
    "                assert(nor_pgd.shape[0] == nr_samples)\n",
    "                assert(nor_aa.shape[0] == nr_samples)\n",
    "                assert(nor_df.shape[0] == nr_samples)\n",
    "                assert(nor_cw.shape[0] == nr_samples)\n",
    "\n",
    "                nor = [nor_fgsm, nor_bim, nor_pgd, nor_aa, nor_df, nor_cw]\n",
    "                adv = [adv_fgsm, adv_bim, adv_pgd, adv_aa, adv_df, adv_cw]\n",
    "\n",
    "                for source in range(len(nor)):\n",
    "                    X_train_src, y_train_src, X_test_src, y_test_src = prepare_datasets(nor[source], adv[source], factor)\n",
    "                    clf_lr, scaler, y_hat_lr, y_hat_pr_lr = LR(X_train_src, y_train_src, X_test_src, y_test_src)\n",
    "                    clf_rf, y_hat_rf, y_hat_pr_rf = RF(X_train_src, y_train_src, X_test_src, y_test_src)\n",
    "\n",
    "                    for target in range(len(nor)):\n",
    "                        if source == target:\n",
    "                            continue\n",
    "                        \n",
    "                        X_train_tar, y_train_tar, X_test_tar, y_test_tar = prepare_datasets(nor[target], adv[target], factor)\n",
    "                        \n",
    "                        y_test_lr, y_hat_pr_lr = predictions(clf_lr, X_test_tar, scaler)\n",
    "                        auc_lr = round(100*roc_auc_score(y_test_lr, y_hat_pr_lr), 2)\n",
    "                        f1_lr =  round(100*f1_score(y_test_lr, y_hat_lr), 2)\n",
    "\n",
    "                        y_test_rf, y_hat_pr_rf = predictions(clf_rf, X_test_tar)\n",
    "                        auc_rf = round(100*roc_auc_score(y_test_rf, y_hat_pr_rf), 2)\n",
    "                        f1_rf =  round(100*f1_score(y_test_rf, y_hat_rf), 2)\n",
    "                        \n",
    "                        if not detector in results[it][dataset][model]:\n",
    "                            results[it][dataset][model][detector] = {}\n",
    "\n",
    "                        if not attacks[source] in results[it][dataset][model][detector]: \n",
    "                            results[it][dataset][model][detector][attacks[source]] = {}\n",
    "\n",
    "                        if not attacks[target] in results[it][dataset][model][detector][attacks[source]]: \n",
    "                            results[it][dataset][model][detector][attacks[source]][attacks[target]] = {}\n",
    "            \n",
    "                        results[it][dataset][model][detector][attacks[source]][attacks[target]]['auc_lr'] = auc_lr\n",
    "                        results[it][dataset][model][detector][attacks[source]][attacks[target]]['auc_rf'] = auc_rf\n",
    "                        results[it][dataset][model][detector][attacks[source]][attacks[target]]['f1_lr']  = f1_lr\n",
    "                        results[it][dataset][model][detector][attacks[source]][attacks[target]]['f1_rf']  = f1_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e74111-ee97-4fb9-bf13-23c6725d54fa",
   "metadata": {},
   "source": [
    "# LID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c988c47-03b7-4e63-b962-076cee1aadc6",
   "metadata": {},
   "source": [
    "# multiLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "20c36627-e22f-464b-ba83-339dc3e91c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_attacks = ['fgsm', 'bim', 'pgd', 'aa', 'df', 'cw']\n",
    "source_attacks = ['fgsm', 'bim']\n",
    "target_attacks = ['fgsm', 'bim', 'pgd', 'aa', 'df', 'cw']\n",
    "models = ['wrn28-10', 'vgg16', 'wrn50-2']\n",
    "datasets =  ['cifar10', 'cifar100', 'imagenet']\n",
    "\n",
    "lid_list_mean = []\n",
    "\n",
    "for it_mean in range(nr_mean):\n",
    "    lid_list = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            if dataset in ['cifar10', 'cifar100'] and model in ['wrn50-2']:\n",
    "                continue\n",
    "            if dataset in ['imagenet'] and model in  ['wrn28-10', 'vgg16']:\n",
    "                continue\n",
    "            \n",
    "            for source_attack in source_attacks:\n",
    "                tmp = np.zeros((len(target_attacks)-1, 2))\n",
    "                iter_tar = 0\n",
    "                \n",
    "                for target_attack in target_attacks:\n",
    "                    if target_attack == source_attack:\n",
    "                        continue\n",
    "                        \n",
    "                    for it, val in enumerate(results[it_mean][dataset][model]['multilid'][source_attack][target_attack].items()):\n",
    "                        tmp[iter_tar, it] = val[1]\n",
    "                    iter_tar += 1\n",
    "                \n",
    "                lid_list.append(tmp.copy())\n",
    "    lid_list_mean.append(np.stack(lid_list))\n",
    "\n",
    "lid_mean = np.mean(lid_list_mean,axis=0)\n",
    "lid_var = np.var(lid_list_mean,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "35625be0-01cb-4f26-b5de-eaf79a54b2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5, 2)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lid_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "7952b528-5a65-4a47-a063-3f75742c7f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bim', 'pgd', 'aa', 'df', 'cw']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacks_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "07297169-a9bd-450b-8595-6794f87ab1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgsm ['bim', 'pgd', 'aa', 'df', 'cw']\n",
      "\\textbf{FGSM} & \\textbf{BIM}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{FGSM} & \\textbf{PGD}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{FGSM} & \\textbf{AA}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{FGSM} & \\textbf{DF}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{FGSM} & \\textbf{CW}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\n",
      "\n",
      "\n",
      "bim ['fgsm', 'pgd', 'aa', 'df', 'cw']\n",
      "\\textbf{BIM} & \\textbf{FGSM}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{BIM} & \\textbf{PGD}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{BIM} & \\textbf{AA}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{BIM} & \\textbf{DF}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\\textbf{BIM} & \\textbf{CW}$80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ & $80.32 \\pm 0.00$ & $70.03 \\pm 0.00$ \\\\\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_tables = []\n",
    "\n",
    "for it_src, source in enumerate(source_attacks):\n",
    "    latex_table = \"\"\n",
    "    attacks_saved = target_attacks.copy()\n",
    "    attacks_saved.remove(source)\n",
    "    print(source, attacks_saved)\n",
    "    for j in range(lid_mean.shape[1]):\n",
    "        latex_table += \"\\\\textbf{\"+f\"{source.upper()}\"+\"} & \\\\textbf{\"+f\"{attacks_saved[j].upper()}\"+\"}\"\n",
    "        for i in range(lid_mean.shape[1]):\n",
    "            mean = lid_mean[it_src*len(attacks_saved)+i, j, 0]\n",
    "            variance = lid_var[i, j, 0]\n",
    "            latex_table += f\"${mean:.2f} \\\\pm {variance:.2f}$ & \"\n",
    "            mean = lid_mean[it_src*len(attacks_saved)+i, j, 1]\n",
    "            variance = lid_var[i, j, 1]\n",
    "            latex_table += f\"${mean:.2f} \\\\pm {variance:.2f}$ & \"\n",
    "        latex_table = latex_table[:-2] + \"\\\\\\\\\\n\"\n",
    "    latex_table += \"\\n\\n\"\n",
    "    print(latex_table)\n",
    "    latex_tables.append(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efcd5d5-7ecd-4b1b-bddd-77ae67250994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cuda--11-1-1--pytorch--1-9-0]",
   "language": "python",
   "name": "conda-env-.conda-cuda--11-1-1--pytorch--1-9-0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
