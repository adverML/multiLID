{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caaf1fe6-cf7c-45bd-9e36-2ba30b647767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lorenzp/workspace/multiLID/data/extract/run_1/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "# pd.set_option(\"max_columns\", None)\n",
    "# pd.set_option('max_colwidth', None)\n",
    "from collections import defaultdict\n",
    "\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "base_path = os.path.join(home, 'workspace/multiLID/data/extract/run_1/')\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97737fb9-6724-488f-9875-69defa3489ce",
   "metadata": {},
   "source": [
    "# from .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1715e5a3-971e-4b95-a685-20cb399cddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.exceptions.ConvergenceWarning('ignore')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf41587-7ab5-4cb0-a7c2-ccd51d3b169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(characteristics, characteristics_adv, noise, test_size=0.2, random_state=42):\n",
    "    \n",
    "    shape_adv = np.shape(characteristics_adv)[0]\n",
    "    shape_char = np.shape(characteristics)[0]\n",
    "    \n",
    "    adv_X_train_val, adv_X_test, adv_y_train_val, adv_y_actual = train_test_split(characteristics_adv, np.ones(shape_adv),   test_size=test_size, random_state=random_state)\n",
    "    b_X_train_val, b_X_test, b_y_train_val, b_y_actual         = train_test_split(characteristics,     np.zeros(shape_char), test_size=test_size, random_state=random_state)\n",
    "    adv_X_train, adv_X_val, adv_y_train, adv_y_val             = train_test_split(adv_X_train_val,     adv_y_train_val,      test_size=test_size, random_state=random_state)\n",
    "    b_X_train, b_X_val, b_y_train, b_y_val                     = train_test_split(b_X_train_val,       b_y_train_val,        test_size=test_size, random_state=random_state)\n",
    "\n",
    "    adv_X_train, adv_X_val, adv_y_train, adv_y_val           = train_test_split(adv_X_train_val,     adv_y_train_val,      test_size=test_size, random_state=random_state)\n",
    "    b_X_train, b_X_val, b_y_train, b_y_val                   = train_test_split(b_X_train_val,       b_y_train_val,        test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "    X_train = np.concatenate(( b_X_train, adv_X_train) )\n",
    "    y_train = np.concatenate(( b_y_train, adv_y_train) )\n",
    "\n",
    "    X_test = np.concatenate( (b_X_test, adv_X_test, b_X_val, adv_X_val) )\n",
    "    y_actual = np.concatenate( (b_y_actual, adv_y_actual, b_y_val, adv_y_val) )\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e99da90-a8cf-45b3-b767-5fd4847865dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(X_train, y_train, X_test, y_test):\n",
    "    scaler  = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    clf = RandomForestClassifier(n_estimators=300, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat =    clf.predict(X_test)\n",
    "    y_hat_pr = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return y_hat, y_hat_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd26d14c-54a5-4043-8ca6-98aec21d2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X_train, y_train, X_test, y_test):\n",
    "    scaler  = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat =    clf.predict(X_test)\n",
    "    y_hat_pr = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return y_hat, y_hat_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3adef36-68cc-45ee-8a2b-167061d1de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig(stacked_rf, mean_rf, var_rf, stacked_lr, mean_lr, var_lr):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlabel(\"$k$\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.xticks(ks)\n",
    "\n",
    "    plt.title(\"APGD-CE $L_{2}$ - LID - AUC values per $k$\")\n",
    "\n",
    "    for iter2, ep in enumerate(epsilonl2):\n",
    "        plt.fill_between([3,5,10,30,50], mean_rf[:,iter2]-var_rf[:,iter2], mean_rf[:,iter2]+var_rf[:,iter2], alpha=0.2, color=colors[iter2])\n",
    "        plt.plot([3,5,10,30,50], mean_rf[:,iter2], marker='o', markersize=5, alpha=0.5, color=colors[iter2], label=epsilonl2_labels[iter2])\n",
    "\n",
    "    # legend = ax.legend( bbox_to_anchor=(1.0, 0.75), title='RF ($\\epsilon$)', loc=6, framealpha=0.3)\n",
    "    legend = ax.legend( bbox_to_anchor=(1.0, 0.75), title='LID', loc=6, framealpha=0.3)\n",
    "    plt.gca().add_artist(legend)\n",
    "\n",
    "    for iter2, ep in enumerate(epsilonl2):\n",
    "        plt.fill_between([3,5,10,30,50], mean_lr[:,iter2]-var_lr[:,iter2], mean_lr[:,iter2]+var_lr[:,iter2], alpha=0.2, color=colors[iter2] )\n",
    "        plt.plot([3,5,10,30,50], mean_lr[:,iter2], marker='v', markersize=5, alpha=0.5, color=colors[iter2], label=epsilonl2_labels[iter2])\n",
    "\n",
    "    #legend2 = plt.legend(saved[0] + saved[1]+ saved[2]+ saved[3]+ saved[4], epsilonl2_labels, bbox_to_anchor=(1.0, 0.3),  title='LR ($\\epsilon$)',  loc=6, framealpha=0.3)\n",
    "    legend2 = plt.legend(saved[0] + saved[1]+ saved[2]+ saved[3]+ saved[4], epsilonl2_labels, bbox_to_anchor=(1.0, 0.3),  title='multiLID',  loc=6, framealpha=0.3)\n",
    "    # plt.gca().add_artist(legend2)\n",
    "\n",
    "    plt.grid(which='major', alpha=0.2)\n",
    "\n",
    "\n",
    "#plt.savefig(\"plots/lines_over_k/fill_apgd-ce_L2.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1487e211-76f5-4156-a0cb-4cbe3ee50aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_SAMPLES=2000\n",
    "epsilonl2=['0.1', '0.2', '0.3', '0.4', '0.5']\n",
    "epsilonl2_labels  = ['0.1', '0.2', '0.3', '0.4', '0.5' ]\n",
    "epsiloninf=[ '05255', '1255', '2255', '4255' , '8255']\n",
    "colors=['blue', 'orange', 'green', 'red', 'purple']\n",
    "length=0\n",
    "eps=['01', '02', '03', '04', '05']\n",
    "\n",
    "attacks = ['apgd-ce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e875809f-541b-42fd-9d92-8a7527a38e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auc(eps, attacks, dataset='cifar10', method='LID', lid_ks=['3', '5', '10', '30', '50']):\n",
    "    dict_res_rf = {}\n",
    "    dict_res_lr = {}\n",
    "    \n",
    "    if '0.1' in eps:\n",
    "        norm = 'l2'\n",
    "    else:\n",
    "        norm = 'linf'\n",
    "        \n",
    "    if dataset == 'cifar10':\n",
    "        ds = 'cifar10/wrn28-10'\n",
    "    else:\n",
    "        ds = 'imagenet/wrn50-2'\n",
    "    \n",
    "\n",
    "    table_rf = np.zeros((len(eps), len(lid_ks)))\n",
    "    table_lr = np.zeros((len(eps), len(lid_ks)))\n",
    "\n",
    "    for att in attacks:\n",
    "        auc_list_rf = []\n",
    "        auc_list_lr = []\n",
    "        for run_nr in [1,2,3]:\n",
    "            print(\"attack\", att)\n",
    "            random_state = np.random.randint(0, 100)\n",
    "            for i_eps, ep in enumerate(eps):\n",
    "                for j_eps, lid_k in enumerate(lid_ks):\n",
    "                    curr_eps = \"\"\n",
    "                    if att in ['apgd-ce', 'fgsm', 'bim', 'pgd', 'aa']:\n",
    "                        curr_eps = \"_\" + ep\n",
    "                    \n",
    "                    clean     = torch.load(os.path.join(base_path.replace('run_1', 'run_{}'.format(run_nr)), ds + '/{}/{}/k{}/{}_normalos_apgd-ce_{}{}.pt'.format(method.lower(), att, lid_k, method.lower(), norm, curr_eps)) ).numpy()[:NR_SAMPLES]\n",
    "                    chars_adv = torch.load(os.path.join(base_path.replace('run_1', 'run_{}'.format(run_nr)), ds + '/{}/{}/k{}/{}_adverlos_apgd-ce_{}{}.pt'.format(method.lower(), att, lid_k, method.lower(), norm, curr_eps)) ).numpy()[:NR_SAMPLES]\n",
    "\n",
    "                    characteristics_re     = clean.reshape((clean.shape[0], -1))\n",
    "                    characteristics_adv_re = chars_adv.reshape((chars_adv.shape[0], -1))\n",
    "\n",
    "                    X_train, y_train, X_test, y_test = split_data(characteristics_re, characteristics_adv_re, noise=False, test_size=0.2, random_state=random_state)\n",
    "\n",
    "                    y_hat, y_hat_pr = RF(X_train, y_train, X_test, y_test)\n",
    "                    auc = round(100*roc_auc_score(y_test, y_hat_pr), 2)\n",
    "                    table_rf[i_eps,j_eps] = auc\n",
    "\n",
    "                    y_hat, y_hat_pr = LR(X_train, y_train, X_test, y_test)\n",
    "                    auc = round(100*roc_auc_score(y_test, y_hat_pr), 2)\n",
    "                    table_lr[i_eps,j_eps] = auc\n",
    "\n",
    "            auc_list_rf.append(table_rf.copy())\n",
    "            auc_list_lr.append(table_lr.copy())\n",
    "\n",
    "        dict_res_rf[att] = auc_list_rf\n",
    "        dict_res_lr[att] = auc_list_lr\n",
    "    \n",
    "    stacked_rf = np.stack(dict_res_rf[att])\n",
    "    mean_rf = np.mean(stacked_rf, axis=0)\n",
    "    var_rf = np.var(stacked_rf, axis=0)\n",
    "    \n",
    "    stacked_lr = np.stack(dict_res_lr[att])\n",
    "    mean_lr = np.mean(stacked_lr, axis=0)\n",
    "    var_lr = np.var(stacked_lr, axis=0)\n",
    "    \n",
    "    return stacked_rf, mean_rf, var_rf, stacked_lr, mean_lr, var_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c9d39b-809b-4404-9aeb-d1710242805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack apgd-ce\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/lorenzp/workspace/multiLID/data/extract/run_1/imagenet/wrn50-2/lid/apgd-ce/k50/lid_normalos_apgd-ce_linf_1255.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lid_l2_stacked_rf, lid_l2_mean_rf, lid_var_rf, lid_l2_stacked_lr, lid_l2_mean_lr, lid_l2_var_lr \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_auc\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsiloninf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapgd-ce\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlid_ks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m30\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mcreate_auc\u001b[0;34m(eps, attacks, dataset, method, lid_ks)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m att \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapgd-ce\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfgsm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpgd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maa\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     29\u001b[0m     curr_eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ep\n\u001b[0;32m---> 31\u001b[0m clean     \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_nr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/k\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_normalos_apgd-ce_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlid_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_eps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()[:NR_SAMPLES]\n\u001b[1;32m     32\u001b[0m chars_adv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(run_nr)), ds \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/k\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_adverlos_apgd-ce_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(method\u001b[38;5;241m.\u001b[39mlower(), att, lid_k, method\u001b[38;5;241m.\u001b[39mlower(), norm, curr_eps)) )\u001b[38;5;241m.\u001b[39mnumpy()[:NR_SAMPLES]\n\u001b[1;32m     34\u001b[0m characteristics_re     \u001b[38;5;241m=\u001b[39m clean\u001b[38;5;241m.\u001b[39mreshape((clean\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/cuda--11-1-1--pytorch--1-9-0/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/cuda--11-1-1--pytorch--1-9-0/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/cuda--11-1-1--pytorch--1-9-0/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/lorenzp/workspace/multiLID/data/extract/run_1/imagenet/wrn50-2/lid/apgd-ce/k50/lid_normalos_apgd-ce_linf_1255.pt'"
     ]
    }
   ],
   "source": [
    "lid_l2_stacked_rf, lid_l2_mean_rf, lid_var_rf, lid_l2_stacked_lr, lid_l2_mean_lr, lid_l2_var_lr = create_auc(eps=epsiloninf, dataset='imagenet', attacks=['apgd-ce'], method='LID', lid_ks=['3', '5', '10', '30', '50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bb97b-7f96-4bf0-bb46-1d2924829a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lid_l2_stacked_rf, lid_l2_mean_rf, lid_var_rf, lid_l2_stacked_lr, lid_l2_mean_lr, lid_l2_var_lr = create_auc(eps=['0.1', '0.2', '0.3', '0.4', '0.5'], dataset='imagenet', attacks=['apgd-ce'], method='LID', lid_ks=['3', '5', '10', '30', '50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14744929-ee1d-403e-aa27-2f579d0dcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig(lid_l2_stacked_rf, lid_l2_mean_rf, lid_var_rf, lid_l2_stacked_lr, lid_l2_mean_lr, lid_l2_var_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca6538-af1e-43f5-abdb-6ab6519340d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mLid_l2_stacked_rf, mLid_l2_mean_rf, mLid_var_rf, mLid_l2_stacked_lr, mLid_l2_mean_lr, mLid_l2_var_lr = create_auc(eps=['01', '02', '03', '04', '05'], attacks=['apgd-cel2'], method='multiLID', lid_ks=['3', '5', '10', '30', '50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc2802-ef60-4af4-ac64-9afd72f38321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cuda--11-1-1--pytorch--1-9-0]",
   "language": "python",
   "name": "conda-env-.conda-cuda--11-1-1--pytorch--1-9-0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
